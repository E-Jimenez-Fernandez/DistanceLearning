% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/weights_from_nn.R
\name{weights_from_nn}
\alias{weights_from_nn}
\title{Machine-learning weights using neural networks}
\usage{
weights_from_nn(Z, ci_init, epsilon = 1e-06, size = 3, maxit = 200, ...)
}
\arguments{
\item{Z}{A numeric matrix or data frame where each row represents a unit
and each column corresponds to an indicator.}

\item{ci_init}{A numeric vector representing the initial composite indicator
for each unit. Typically produced by \code{initial_index_pnorm()} or a
previous iteration of the DL2 fixed-point algorithm.}

\item{epsilon}{A small positive constant used to guarantee strictly positive
weights and prevent numerical instability. Defaults to \code{1e-6}.}

\item{size}{Integer specifying the number of hidden units in the neural
network. Defaults to \code{3}.}

\item{maxit}{Maximum number of iterations (epochs) for training the network.
Defaults to \code{200}.}

\item{...}{Additional arguments passed to \code{nnet::nnet()}.}
}
\value{
A numeric vector of strictly positive, normalized weights of length
equal to the number of indicators. The weights sum to one.
}
\description{
Computes a vector of indicator weights using a feed-forward neural network
trained to predict the initial composite indicator \code{ci_init} from the
individual indicators. This approach provides a nonlinear, data-driven
weighting scheme within the DL2 methodology, complementing Random Forest
and MARS alternatives.
}
\details{
The function fits a neural network using \code{nnet::nnet()} with a single
hidden layer and linear output activation (\code{linout = TRUE}). The raw
importance of each indicator is derived from the absolute values of the
learned network weights:

\deqn{
  w_j \propto \max(|\theta_j|, \epsilon),
}

where \eqn{\theta_j} denotes the connection weight associated with indicator
\eqn{j}.

Since the internal representation of weights in \code{nnet::nnet()} is a
flattened vector, the function extracts the elements corresponding to the
input layer. If the number of extracted elements does not match the number
of indicators, padding or truncation is applied to ensure dimensional
consistency.

Neural network weights capture nonlinear feature contributions and potential
interactions among indicators, offering a flexible alternative to tree-based
and spline-based methods in the DL2 weighting scheme.
}
\examples{
\dontrun{
set.seed(123)
Z <- matrix(runif(40), nrow = 8)
ci0 <- initial_index_pnorm(Z)
w_nn <- weights_from_nn(Z, ci_init = ci0, size = 4)
w_nn
}

}
